\documentclass{article}
\pagestyle{empty}
\usepackage{enumitem}
\usepackage[font=bf]{caption}
\usepackage{float}
\usepackage{sectsty}
\usepackage[margin=0.9in]{geometry}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{amsmath,amssymb}
\usepackage[colorlinks, linkcolor={red!70!black}]{hyperref}
\usepackage[most]{tcolorbox}

\sectionfont{\fontsize{12}{15}\selectfont}

\begin{document}
\section{System Goals}
\par Many recent transactional database systems, such as Google's Spanner, FaunaDB, and AWS DynamoDB, guarantee Strict Serializability (or stronger) for application programmers. Our system \emph{guarantees Regular Sequential Serializability}, a consistency model that weakens Strict Serializability but maintains the same set of application invariants.
\par Furthermore, both Strict Serializability and Regular Sequential Serializability assume synchronous client interaction. That is, their guarantees only hold for complete operations -- those with a matching invocation and response pair. This system aims to provide \emph{stronger semantics for multiple outstanding client operations} in the form of a invocation-order guarantee. As the name suggests, all clients can expect system execution to respect the order of their invocations. 
\par Performance wise, we strive to achieve \emph{high throughput}, minimizing the overhead imposed by providing the invocation-order guarantee. Following the approach of Calvin, we ``determinize'' the order of transactions through a shared log, eliminating reliance on concurrency control mechanisms in the common case. We also emphasize \emph{data durability}; once returned to the client, a transaction will not be lost unless an improbable number of service components fail. 

\section{Client-Service Interaction}
\par Clients establish a session with the service using a local library, through which all interactions occur. The client-side library transparently maintains this session and retries requests without the need for user intervention. Clients may have multiple outstanding transaction requests and can expect that the resulting execution preserves invocation order. Read-only transactions are automatically assigned a sequence number respecting real-time invocation order, which we call a CRSN (client read-only sequence number). Similarly, read-write transactions and write-only transactions are stamped with a CWSN (client write sequence number). Both CRSNs and CWSNs are composed of a unique client identifier, $cid$, and a number $num \in \mathbb{N}$ i.e. $(cid, 0), (cid, 1), (cid, 2),...$ where $(cid, 0) < (cid, 1) < (cid, 2) < ...$ with respect to invocation order.
\section{System Assumptions and Design Overview}
\par In our system, all transactions are handled by a transaction manager, which is a collection of servers ordered to form a chain. This manager communicates with shard groups to durably store data. Each shard group is a multi-versioned, linearizable, storage service. Only read-write transactions are replicated in the manager chain, and clients can open a session with any manager server \textit{except for} the head and tail. 
\par Assume the transaction manager consists of $n$ servers and data is split among $m$ shard groups. Uniquely identify each node in the chain with an index $i \in [1,n]$ based on their location. That is, the head of chain is $1$; the tail is $n$; and the successor of any other node $i$ in the chain is $i+1$. We also label each shard group with some unique $j \in [1, m]$. Figure 1 outlines the data structures present on service nodes and client sessions. 
\par As in Chain Replication and CRAQ, we assume the transaction manager servers are fail-stop. We also assume the presence of a failure detector for the chain and a coordination service, which maintains the mapping from shards to keyspaces. We will refer to this mapping as $sh(\cdot)$. Our network model is asynchronous, and messages can be dropped, delivered out of order, or both. 
\par Read-write transactions start as $PreCommit$ at the head of the chain and are replicated at successive nodes. Once the tail appends a transaction to its log, the transaction is considered committed, and its constituent sub-transactions sent to the relevant shard groups for execution. Completion of sub-transactions are sent to the chain tail. After the tail learns that all of these are complete, a command to transition the transaction to $Executed$ is sent backwards through the chain. A transaction whose status changes to $Executed$ on the head node subseqeuntly has its completion returned to the client.
\par The rest of this specification is as follows. We will detail the protocol for write-only and read-only transactions. Then, we describe how read-only and dependent read-only transactions are reduced to the case of write-only transactions. 
\par As a notational convention, we will take superscript ``write'' to denote the write key-set and superscript ``read'' to denote the read key-set of a transaction.
\begin{figure}[H]
  \vspace*{-2em}
  \centering
\begin{tcolorbox}[title= On \textbf{transaction manager node} $i$, enhanced, width=1.05\textwidth, left skip=-0.5cm, colback=black!4!white]  
\begin{enumerate}[itemsep=1pt]
\item \underline{Write-ahead log of read-write transactions, $\mathcal{L}_i$}: All entries on the log have a sequence number, which doubles as an index into the log, and have a type of $PreCommit$, $Executed$, or $Abort$. We defer discussion of these types to the protocol description. For convenience, let $\tilde{\mathcal{L}_i}$ denote the subhistory (not necessarily suffix) consisting of $PreCommit$ entries and $\tilde{\mathcal{L}_i}|j$ denote the subhistory of $PreCommit$ entries that affect keys belonging to shard group $j$. Observe that $\tilde{\mathcal{L}_i}|j \subseteq \tilde{\mathcal{L}_i}$ and  \begin{equation*}
    order\left(\tilde{\mathcal{L}_i}|j \right) \subseteq order\left(\tilde{\mathcal{L}_i} \right)\subseteq order(\mathcal{L}_i)
  \end{equation*}
  as a consequence of these definitions.\footnote{Here, $order(S)$ is the assumed total order over $S$ -- a distinguished subset of $S \times S$ that satisfies the usual axioms. The natural order on log sequence numbers, inherited from $\mathbb{N}$, induces all orderings we consider herein.}
\item \underline{FIFO queue for each shard group $j$, $Q_i|j$}: Each queue is maintained so that its contents and order are exactly the sequence numbers of $\tilde{\mathcal{L}_i}|j$. Additionally, each queue tracks the log sequence number of the most recently dequeued entry, which we will write as $exec(Q_i|j)$.
\item \underline{Map from $PreCommit$ transactions to shards, $C_i$}: Any given $PreCommit$ transaction $T \in \mathcal{L}$ is viewed as a collection of disjoint sub-transactions $\{T|j_p\}_{p=1}^k$, where $k \leq m$ and $T|j_p$ is the subset of writes in $T$ that only mutate the state of shard $j_p$. This map stores bindings of the form $T.sn \mapsto (shds, cwsn)$, where $shds  :=\{j_1,...,j_k\}$ and $cwsn$ is the associated CWSN. For a given $PreCommit$ transaction, $shds$ informs us which participating shard groups have not yet executed the necessary writes. The $cwsn$ enables the update of transaction status in item 5. 
\item \underline{Map from shards to sequence number, $SSN_i$}: In the same way that the client library appends each outgoing transaction with either a CWSN or CRSN, the manager nodes increment $SSN$ on every insert into $Q_i|j$. 
\item \underline{Map from CWSNs to transactions, $W_i$}: This associates CWSNs with either their corresponding ongoing write or write response that has not been acknowledged. Write $W_i^{\text{key}}$ to represent the domain (keys) of the mapping and $W_i^{\text{value}}$ be the range. Define $W_i^{\text{key}}|cid$ as the maximal subset of $W_i^{\text{key}}$ such that all of its elements have prefix $cid$, and let $\max(W_i^{\text{key}}|cid)$ be its maximal element with respect to the ordering on CWSNs. If $W_i^{\text{key}}|cid = \emptyset$, then $\max(W_i^{\text{key}}|cid) = -1$. This additional per-client bookkeeping is essential to guaranteeing exactly-once semantics in the presence of server failures and message loss (\S \ref{wotxn}).
\item \underline{Map from $cid$s to log sequence numbers, $R_i$}: As we will see in \S \ref{rotxn}, this other form of per-client metadata is used for preserving the invocation-order for reads. 
\end{enumerate}
\end{tcolorbox}
\begin{tcolorbox}[title= On \textbf{shard group} $j$, enhanced,width=1.05\textwidth, left skip=-0.5cm, colback=black!4!white]
\begin{enumerate}[itemsep=1pt]
\item \underline{Log sequence number of the most recently completed sub-transaction, $shExec(j)$}: This is analous to the queues and $exec(\cdot)$ in the transaction manager nodes. 
\item \underline{Local sequence number, $ssn_j$}: After the completion of any sub-transaction $T|j$, the shard group increments $ssn_j$. 
\end{enumerate}
\end{tcolorbox}
\begin{tcolorbox}[title= On \textbf{client} $cid$, enhanced,width=1.05\textwidth, left skip=-0.5cm, colback=black!4!white]
  \begin{enumerate}[itemsep=1pt]
  \item \underline{Greatest assigned CWSN and CRSN, $cwsn_{\max}$ and $crsn_{\max}$}: These are incremented accordingly as the client submits transactions.
  \item \underline{Ordered set of CWSNs assigned to outstanding transactions, $InProg_{cid}$:} This allows computation of the largest CWSN such that all transactions with lower CWSNs have recieved responses (\S \ref{wotxn}). Set elements are removed on receiving responses. 
  \item \underline{Ordered map from CRSNs to log sequence numbers, $UB_{cid}$}: This is a set of upper bounds that ensure the results of a retried read-only transaction do not violate invocation ordering (\S \ref{rotxn}). As above, the client library prunes this map appropriately as it recieves responses from the service. 
  \end{enumerate}
\end{tcolorbox}
\caption{Data structures on transaction manager, shard group servers, and clients}

\end{figure}



\newpage
\section{Write-Only Transactions} \label{wotxn}
\par We start with the special case of write-only transactions. First, clients invoke $\text{SessionRuntimeAppend}$. \\
\begin{procedure}[H]
  \caption{SessionRuntimeAppend($T$, $isRetry$, $cwsn_T$ $|$   null)}
  \tcp{SessionRuntimeAppend on client $cid$}
  \uIf{$\neg isRetry$}{
    $cwsn_{\max} := cwsn_{\max} + 1$ \\
    $cwsn_T := cwsn_{\max}$\\
  }
  $ackBound := \min(InProg_{cid})$ \\
  $\text{Send}_{\text{head}}(\text{AppendTransact}, \{T, cwsn_T, ackBound\})$ \tcp{all writes go to head of chain} 
  \textbf{return} $cwsn_T$
\end{procedure}
% TODO rewrite 
% Along with the transaction $T$ itself, the client library includes
% \begin{itemize}
% \item The transaction CWSN -- $cswn_T$
% \item The largest CWSN such that all transactions with lower CSWNs have recieved responses -- $ackBound$
% \end{itemize}
Note the variables $cwsn_T$ and $ackBound$. The first of these two guarantees execution preserves invocation order in spite of message reordering, while the second enables the transaction manager to garbage collect no-longer-needed metadata. Additionally, $cwsn_T$ is returned to the client for any possible retries. \\
\begin{procedure}[H]
  \SetKwFor{Match}{match}{with:}{end}
  \caption{AppendTransact($T$, $cwsn_T$, $ackBound$)}
  \tcp{AppendTransact on transaction manager node $i \in [1, n]$} 
  \uIf{$cwsn_T \leq \max(W_i^{\text{key}}|cid)$}{
    \tcp{$W_i[cwsn_T]$ has variant type $\text{resp}|\text{uint}$}
    \Match{$(i, W_i[cwsn_T])$}{ 
      $(1, resp) \implies \text{Send}_{\text{cid}}(\text{SessionResp}, \{resp\})$\tcp{handle case when $i$ is head}
      $(\_, \_) \implies  \text{\textbf{return }}$
    }
  } \Else {
    \textbf{wait} until $\max(W_i^{\text{key}}|cid).num + 1 = cwsn_T.num$     \tcp{prevent reordering} 
  }
  ind := $\mathcal{L}_i.len$ \\
  $\mathcal{L}_i \leftarrow T$ \\
  $W_i := W_i \cup \{ cwsn_T \mapsto \text{ind}\}$ \\
  $C_i := C_i \cup \left \{ind \mapsto (cwsn_T, \{ \}) \right \}$ \\
  \ForEach{$j\in[1,m]$}{
    particip := $T^{\text{write}} \cap sh(j)$\\
    \uIf{\upshape particip $\neq \emptyset$}{$Q_i|j \leftarrow \text{ind}$ \\
      $SSN[j] := SSN[j] + 1$\\
      $C_i[\text{ind}].shds := C_i[\text{ind}].shds \cup \{j\}$ 
    }
  }
  \uIf{$i = n$}{ 
    \ForEach{\upshape$j \in C_i[\text{ind}]$}{
      $\text{Send}_{j}(\text{ShardExec}, \{T|j, \text{ind}, SSN[j]\})$ 
    }
  } \Else {
    $\text{Send}_{\text{succ}}(\text{AppendTransact}, \{T, cwsn_T\})$ 
  }
  $W_i := W_i \setminus \{W_i[k] : k \in W_i^{\text{key}} \wedge k < ackBound\}$  \tcp{remove unneeded metadata} 
  \textbf{return}
\end{procedure}
\par The check in lines 2-5 is necessary, as a dropped response message can lead to multiple client retries. If the service indiscriminately performs these retries, the resulting execution history might not admit a valid total ordering. 
\par Once the transaction is replicated on the tail node, it is sent to the shard groups for execution using $\text{ShardExec}$. To reduce the size of network messages, only the relevant sub-transaction is transmitted to each participating shard. Similar to before, a shard-specific sequence number $sn$ is also included. Retries are piggybacked on subsequent invocations or sent after a preset timeout, whichever happens first. \\
\begin{procedure}[H]
  \caption{ShardExec($T|j$, $ind$, $sn$)}
  \tcp{ShardExec on shard group $j \in [1, m]$}
  \uIf{$sn \leq ssn_j$}{
    \textbf{return}
  } \Else{
    \textbf{wait} until $ssn_j + 1 = sn$
  }
  \ForEach{$op \in T|j$}{
    \tcp{abstract put exposed by storage service, these calls will be batched in practice}
    put($op.key$, $op.value$)
  }  
  $ssn_j := sn$ \\
  $\text{Send}_{\text{tail}}(\text{ExecNotif}, \{j, ind\})$\\
  \textbf{return}
\end{procedure}
\begin{procedure}[H]
  \caption{ExecNotif($j$, $ind$)}
  \tcp{ExecNotif on tail node $n$}
  $\text{deq} \leftarrow Q_n|j$\\
  \While{\upshape deq $\neq ind$}{
    deq $\leftarrow Q_n|j$
  }
  $C_n[ind].shds = C_n[ind].shds\setminus\{ j\}$\\
  \uIf{$C_n[ind].shds = \emptyset$}{
    $\text{Send}_\text{tail}(TransactExec, \{ind\})$
  }
  \textbf{return}
\end{procedure}\hfill 
\noindent\par The tail listens to shard group replies through $\text{ExecNotif}$ and updates its $C_{n}$. After all sub-transactions are finished, transaction completion is propagated backwards to the head through $\text{TransactExec}$. For each participating shard queue, elements are deqeued up to and including the completed transaction index. This ensures an updated view of all execution statuses, which is critical for correctly serving reads. When the head is done updating its queues, the response is returned to the client session.\\
\begin{procedure}[H]
  \caption{TransactExec($ind$)}
  \tcp{TransactExec on node $i \in [1, n]$}
  $cwsn := C_i[ind].cwsn$\\
  \ForEach{$j \in C_i[ind].shds$}{
    deq $\leftarrow Q_i|j$ \\
    \While{deq $\neq ind$}{
      deq $\leftarrow Q_i | j$
    }
  }
  $C_i := C_i \setminus C_i[ind]$\\ 
  $W_i[cwsn] := \text{resp}\{\text{SUCCESS}\}$\\
  \uIf{$i=1$}{
    $\text{Send}_{\text{cwsn}.cid}(\text{SessionRespWrite}, \{cwsn, W_i[cwsn]\})$
  }
  \Else{
    $\text{Send}_{\text{pred}}(\text{TransasctExec}, \{ind\})$
  }
  \textbf{return}
\end{procedure}
\begin{procedure}[H]
  \caption{SessionRespWrite($cwsn$, $resp$)}
  \tcp{SessionRespWrite on client $cid$}
  $InProg_{cid} := InProg_{cid}\setminus \{cwsn\}$\\
  \textbf{return} $resp$
\end{procedure}
\newpage
\section{Read-Only Transactions} \label{rotxn}
% \par In contrast with write-only transactions, there is no data structure tracking read-only transactions on the transaction manager nodes. Moreover, client sessions . As such, the client-side library plays a non-trivial role in ensuring correctness.
 $\text{SessionRuntimeRead}$ in the client-side library is called on every read-only transaction, including retries.   \\
\begin{procedure}[H]
  \caption{SessionRuntimeRead($T$, $isRetry$, $crsn_T$ $|$ null)}
  \tcp{SessionRuntimeRead on client $cid$, connected to manager node $i$}
  $lsnConst := \emptyset$\\
  \uIf{$\neg isRetry$}{
    $crsn_{\max} := crsn_{\max} + 1$ \\
    $crsn_T := crsn_{\max}$\\
  } \Else{
    ubKey $:= \min\{ x \in UB_{cid}^{\text{key}} : crsn_T\leq x\}$\\
    $lsnConst := \{UB_{cid}[\text{ubKey}]\}$
  }
  $lsnConst := lsnConst \cup \{ \}$ \\
  $ackBound := \min(InProg_{cid})$ \\
  $\text{Send}_{i}(\text{ReadOnlyTransact}, \{T, crsn_T, ackBound, lsnConst\})$ \\
  \textbf{return} $crsn_T$
\end{procedure}
Like write-only transactions, an $ackBound$ notifies the service which responses the client has successfully recieved. Every 

For retries, the client session must also explicitly provide an upper bound on freshness to the transaction manager node. Otherwise, a retry could return information newer than that of later reads, with respect to invocation order. We take the upper bound to be the log sequence number associated . Observe that this scheme can potentially lead to a large amount of .  



% \subsection*{Read-Only Transactions}
% TODO\\
% \textbf{Client Sessions}
% \subsection*{Read-Write Transactions}
% \par For a read-write transaction $T$, both the write-set $T^{\text{write}}$ and read-set $T^{\text{read}}$ are fully known. However, the writes can depend on reads, either for the value being written or for some condition. An example for the former is $T := \{\text{write}(b) = \text{read}(a) + 10 \}$. A classic example involving conditional writes is a bank withdrawl, $T := \{\textbf{if }(\text{read}(a) >= 100) \textbf{ then } \text{write}(a) = \text{read}(a) - 100\}$. 
% \\
% TODO


% \subsection*{Dependent read-write transactions}

% \subsection*{Failure Recovery}
% TODO: Need to talk about failure recovery for outstanding reads and outstanding writes. Aborts replicated for performance on head failover.

% Client first performs read before issuing write. On tail node failover, any committed but not yet executed transactions must be re-processed by the new tail. Since writes are predetermined and buffered at client side, applying the unexecuted suffix of the log is an idempotent operation for all shards.


% \begin{procedure}
%   \caption{ChainNodeApply($T_i$)}
%   \SetKwInput{KwInput}{Args}
%   \SetKwInOut{Output}{Return}
%   \tcp{Same as CRAQ}
%   \tcp{Here, could do some optimization to allow nodes in chain to send shard executes}
% \end{procedure}

% \begin{procedure}
%   \caption{TailCommit($T_i$)}
%   \SetKwInput{KwInput}{Args}
%   \SetKwInOut{Output}{Return}
%   $L^n \leftarrow T_i$\\
%   $\text{Send}_{pred}\left(\text{ChainNodeCommit}, \{T_i\} \right)$\\
%   $Q = \text{Interfere}(T_i)$\\
%   $Q \leftarrow T_i$ \\
%   \textbf{wait} until $head(Q) = T_i$ 
%   \\
%   \ForEach{$op \in T_i$}{
%     $sh\_id = \text{shard}(op)$\\
%     $\text{Send}_{sh\_id}\left(\text{ShardExecute}, \{op\} \right)$
%   } 
% \end{procedure}

% \begin{procedure}
%   \caption{QueueWorker()}
%   TODO \tcp{On recieving acks for all ops in a transaction, remove from queue}
% \end{procedure}

% \begin{procedure}
%   \caption{ShardExecute$_i$($op$)}
%   \SetKwInput{KwInput}{Args}
%   \SetKwInput{Output}{Return}
%   \If{$executed_i \geq op.sn$}{\textbf{return}}
%   \ForEach{$op \in T_i$}{
%     Apply($op$)\\
%     $executed_i = op.sn$ 
%   }
%   $nid \in \{1,...,n \}$ \tcp{Any chain node is fine; this uses config service}
%   $lastOp = \text{tail}(L_i|sh_{id})$ \\
%   $\text{Send}_{nid}\left(\text{AckExecute},  \right)$
% \end{procedure}

% \begin{algorithm}
%   \caption{AckExecute (on any node $nid$)}
%   \SetKwInput{KwInput}{Args}
%   \SetKwInOut{Output}{Return}
%   \KwInput{}
% \end{algorithm}

% How to handle concurrent reads? Two sources of concern here: inconsistent snapshots, and respecting invocation order.
% \\For first concern,
% \begin{itemize}
% \item If we send full log suffixes, just block until all shards reach same s/n as max. Need to read from log layer to prevent key on ``cold shard'' blocking read indefinitely.
% \item Alternatively, client constructs ``interference'' graph based on shard responses. For each connected component, every shard must match maximum sequence number (slow path if this condition not satisfied). 
% \end{itemize}
% For second concern,
% \begin{itemize}
% \item Could add invocation order constraint on ``interference'' graph above, which would be a part of the client library. Each invocation has own local graph, with shared constraint edges. Note that vertices included in constraint edges also need to be added to local graph for bookkeeping purposes (don't actually need to read from that shard). Draw directed edges to enforce partial order (allow self edges). For completed reads, remove relevant edges. 
% \end{itemize}

%\noindent Alternatively, could use Calvin-esque read to write propagation. Same issue here with disk stalls...
%Let the suffix of the log on node $i$ starting from and including sequence number $j$ be written as $L_j^i$. Where convenient (and safe), we will drop the node identifier.
% Denote the log of transaction manager node $i$, as $L^i$, where $1 \leq i \leq n$. Each chain node has variables $lastExec$, which tracks the most recent entry in the log that has been executed by the relevant shards, and $lastCommit$, most recent commited (but not necessarily executed) entry. The assertion $lastExec \leq lastCommit$ should be a protocol invariant. 
%\par Each shard group $1 \leq k \leq m$ maintains $executed_i$, the last log sequence number it has executed. $L_j^i|k$ denotes the subhistory of the log that affects keys belonging to shard group $k$.

%   This is updated such that the CWSNs are one-to-one with $\tilde{\mathcal{L}_i}$.
% \item \underline{Set of unacknowledged write transaction responses}: Concretely, a set of CWSNs whose corresponding transaction responses have not been acknowledged.
\end{document}
