\documentclass{article}
\pagestyle{empty}
\usepackage{enumitem}
\usepackage[font=bf]{caption}
\usepackage{float}
\usepackage{sectsty}
\usepackage[margin=0.9in, top=0.7in]{geometry}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{amsmath,amssymb}
\usepackage[colorlinks, linkcolor={red!70!black}]{hyperref}
\usepackage[most]{tcolorbox}

\sectionfont{\fontsize{12}{15}\selectfont}
\subsectionfont{\fontsize{10.5}{15}\selectfont}

\begin{document}
\section{System Goals}
\par Many recent transactional database systems, such as Google Spanner, FaunaDB, and AWS Aurora, guarantee Strict Serializability (or stronger) for application programmers. Our system \emph{guarantees Regular Sequential Serializability}, a consistency model that weakens Strict Serializability but maintains the same set of application invariants. This allows more flexibility in serving reads, potentially decreasing tail-latency. 
\par Both Strict Serializability and Regular Sequential Serializability assume synchronous client interaction. That is, their guarantees only hold for complete operations -- those with a matching invocation and response pair. This system aims to provide \emph{correctness for multiple outstanding client operations} in the form of a invocation-order guarantee. As the name suggests, all clients can expect system execution to respect the order of their invocations. 
\par Performance wise, we strive to achieve \emph{high throughput}, minimizing the overhead imposed by providing the invocation-order guarantee. Following the approach of Calvin, we ``determinize'' the order of transactions through a shared log, eliminating reliance on concurrency control mechanisms in the common case. We also emphasize \emph{data durability}; once returned to the client, a transaction will not be lost unless an improbable number of service components fail. 

\section{Client-Service Interaction}
\par Clients establish a session with the service using a local library, through which all interactions occur. The client-side library transparently maintains this session without the need for user intervention. Clients may have multiple outstanding transaction requests and can expect that the resulting execution preserves invocation order. Read-only transactions are automatically assigned a sequence number respecting real-time invocation order, which we call a CRSN (client read-only sequence number). Similarly, read-write transactions and write-only transactions are stamped with a CWSN (client write sequence number). Both CRSNs and CWSNs are composed of a unique client identifier, $cid$, and a number $num \in \mathbb{N}$ i.e. $(cid, 0), (cid, 1), (cid, 2),...$ where $(cid, 0) < (cid, 1) < (cid, 2) < ...$ with respect to invocation order.
\section{System Assumptions and Design Overview}
\par In our system, all transactions are handled by a transaction manager, which is a collection of servers ordered to form a chain. This manager communicates with shard groups to durably store data. Each shard group is a multi-versioned, linearizable, storage service. Only read-write transactions are replicated in the manager chain, and clients can open a session with any manager server \textit{except for} the head and tail. 
\par Assume the transaction manager consists of $n$ servers and data is split among $m$ shard groups. Uniquely identify each node in the chain with an index $i \in [1,n]$ based on their location. That is, the head of chain is $1$; the tail is $n$; and the successor of any other node $i$ in the chain is $i+1$. We also label each shard group with some unique $j \in [1, m]$. Figure \ref{state} outlines the data structures present on service nodes and client sessions. 
\par As in Chain Replication and CRAQ, we assume the transaction manager servers are fail-stop. We also assume the presence of a failure detector for the chain and a coordination service, which maintains the mapping from shards to keyspaces. We will refer to this mapping as $sh(\cdot)$. Our network model is asynchronous, and messages can be dropped, delivered out of order, or both. 
\par Read-write transactions start as $PreCommit$ at the head of the chain and are replicated at successive nodes. Once the tail appends a transaction to its log, the transaction is considered committed, and its constituent sub-transactions sent to the relevant shard groups for execution. Completion of sub-transactions are sent to the chain tail. After the tail learns that all of these are complete, a command to transition the transaction to $Executed$ is sent backwards through the chain. A transaction whose status changes to $Executed$ on the head node subsequently has its completion returned to the client.
\par The rest of this specification is as follows. We will detail the protocol for write-only and read-only transactions. Then, we describe how read-only and dependent read-only transactions are reduced to the case of write-only transactions. 
\par As a notational convention, we will take superscript ``write'' to denote the write key-set and superscript ``read'' to denote the read key-set of a transaction. For maps, superscript ``key'' represents the domain of the map and ``value'' to be its range. On maps or sets whose keys admit an ordering, let $\max(\cdot)$ and $\min(\cdot)$ have their usual meaining. If the map or set $S$ is empty, then both $\max(S), \min(S) = -1$. 
\begin{figure}[H]
  \vspace*{-2.5em}
  \centering  
\begin{tcolorbox}[title= On \textbf{transaction manager node} $i$, enhanced, width=1.05\textwidth, left skip=-0.5cm, colback=black!4!white]  
\begin{enumerate}[itemsep=1pt]
\item \underline{Write-ahead log of read-write transactions, $\mathcal{L}_i$}: All entries on the log have a sequence number, which doubles as an index into the log, and have a type of $PreCommit$, $Executed$, or $Abort$. We defer discussion of these types to the protocol description. For convenience, let $\tilde{\mathcal{L}_i}$ denote the subhistory (not necessarily suffix) consisting of $PreCommit$ entries and $\tilde{\mathcal{L}_i}|j$ denote the subhistory of $PreCommit$ entries that affect keys belonging to shard group $j$. Observe that $\tilde{\mathcal{L}_i}|j \subseteq \tilde{\mathcal{L}_i}$ and  \begin{equation*}
    order\left(\tilde{\mathcal{L}_i}|j \right) \subseteq order\left(\tilde{\mathcal{L}_i} \right)\subseteq order(\mathcal{L}_i)
  \end{equation*}
  as a consequence of these definitions.\footnote{Here, $order(S)$ is the assumed total order over $S$ -- a distinguished subset of $S \times S$ that satisfies the usual axioms. The natural order on sequence numbers, inherited from $\mathbb{N}$, induces all orderings we consider herein.}
\item \underline{FIFO queue for each shard group $j$, $Q_i|j$}: Each queue is maintained so that its contents and order are exactly the sequence numbers of $\tilde{\mathcal{L}_i}|j$. Additionally, each queue tracks the log sequence number of the most recently dequeued entry, which we will write as $exec(Q_i|j)$.
\item \underline{Map from $PreCommit$ transactions to shards, $C_i$}: Any given $PreCommit$ transaction $T \in \mathcal{L}$ is viewed as a collection of disjoint sub-transactions $\{T|j_p\}_{p=1}^k$, where $k \leq m$ and $T|j_p$ is the subset of operations in $T$ that only concern the state on shard $j_p$. This map stores bindings of the form $T.sn \mapsto (shds, cwsn)$, where $shds  :=\{j_1,...,j_k\}$ and $cwsn$ is the associated CWSN. For a given $PreCommit$ transaction, $shds$ informs us which participating shard groups have not yet executed the necessary operations, and the $cwsn$ enables the update of transaction status in item 5. 
\item \underline{Map from shards to sequence number, $SSN_i$}: In the same way that the client library appends each outgoing transaction with either a CWSN or CRSN, the manager nodes increment $SSN$ on every insert into $Q_i|j$. 
\item \underline{Map from CWSNs to transactions, $W_i$}: This associates a CWSN with either $\text{null}$ or its corresponding response $(status, data)$ that has not yet been acknowledged. Define $W_i^{\text{key}}|cid$ as the maximal subset of $W_i^{\text{key}}$ such that all of its elements have prefix $cid$. This additional per-client bookkeeping is essential to guaranteeing exactly-once semantics in the presence of server failures and message loss (\S \ref{wotxn}).
\item \underline{Map from $cid$s to $(maxNum, lsn)$ tuple, $R_i$}: The component $maxNum$ is the highest CSRN $num$ seen from client $cid$, and $lsn$ is a log sequence number. As we will see in \S \ref{rotxn}, this other form of per-client metadata is used for preserving the invocation-order for reads when messages arrive out of order. 
\end{enumerate}
\end{tcolorbox}
\begin{tcolorbox}[title= On \textbf{shard group} $j$, enhanced,width=1.05\textwidth, left skip=-0.5cm, colback=black!4!white]
\begin{enumerate}[itemsep=1pt]
\item \underline{Log sequence number of the most recently completed sub-transaction, $shExec_j$}: This is analogous to the queues and $exec(\cdot)$ in the transaction manager nodes. 
\item \underline{Local sequence number, $ssn_j$}: After the completion of any sub-transaction $T|j$ that involves a state mutation, the shard group increments $ssn_j$. 
\end{enumerate}
\end{tcolorbox}
\begin{tcolorbox}[title= On \textbf{client} $cid$, enhanced,width=1.05\textwidth, left skip=-0.5cm, colback=black!4!white]
  \begin{enumerate}[itemsep=1pt]
  \item \underline{Greatest assigned CWSN and CRSN, $cwsn_{\max}$ and $crsn_{\max}$}: These are incremented accordingly as the client submits transactions (\S \ref{wotxn}, \ref{rotxn}).
  \item \underline{Ordered set of CWSNs assigned to outstanding transactions, $InProg_{cid}$:} This allows computation of the largest CWSN such that all transactions with lower CWSNs have received responses (\S \ref{wotxn}). Set elements are removed on receiving responses. 
  \item \underline{Ordered map from CRSNs to log sequence numbers, $UB_{cid}$}: This is a set of upper bounds that ensure the results of a retried read-only transaction do not violate invocation ordering (\S \ref{rotxn}). As above, the client library prunes this map appropriately as it receives responses from the service.
  \item \underline{Map from CRSNs to response data, $ReadResult_{cid}$}: This map is of the form $\{crsn_T \mapsto (num, data, lsn)\}$, where $num$ is the number of participating shards, $data$ contains response data, and $lsn$ is the read's log sequence number constraint (\S \ref{rotxn}). 
  \end{enumerate}
\end{tcolorbox}
\caption{Data structures on transaction manager, shard group servers, and clients}
 \label{state}
\end{figure}



\newpage
\section{Simple Read-Write Transactions} \label{wotxn}
% Should probably write out description in text at some point
\par We distinguish between two types of read-write transactions: \emph{simple} read-write transactions or \emph{dependent} read-write transactions. For now, we focus on simple read-write transactions; dependent read-write transactions are discussed in section \S \ref{deptxn}. Simple read-write transactions do not rely on the results of any constituent operations to determine the full read/write key-set. For example, there are no dependencies amongst the operations in $T := \{ w(x) = 5, r(y), r(z)\}$, so this is of simple type. Another example is a conditional write such as $T:=\{\text{if } r(z) \geq 100 \text{ then } w(x) = x - 100 \}$. While the write to $x$ might depend on $z$, all keys in the transaction are known.
\par To submit a simple read-write transaction to the system, the client invokes $\text{SessionRuntimeAppend}$ through a wrapper API, which handles timeout and retries. \\
\begin{procedure}[H]
  \caption{SessionRuntimeAppend($T$, $isRetry$, $cwsn_T$ $|$   null)}
  \tcp{SessionRuntimeAppend on client $cid$}
  \uIf{$\neg isRetry$}{
    $cwsn_{\max} := cwsn_{\max} + 1$ \\
    $cwsn_T := cwsn_{\max}$\\
  }
  $ackBound := \min(InProg_{cid})$  \tcp{could optimize this to clean specific intervals}
  $\text{Send}_{\text{head}}(\text{AppendTransact}, \{T, cwsn_T, ackBound\})$ \tcp{all writes go to head of chain} 
  \textbf{return} $cwsn_T$
\end{procedure}
Note the variables $cwsn_T$ and $ackBound$. The first of these two guarantees execution preserves invocation order in spite of message reordering, while the second enables the transaction manager to garbage collect no-longer-needed metadata. Additionally, $cwsn_T$ is returned to the client for any possible retries. \\
\begin{procedure}[H]  
  \caption{AppendTransact($T$, $cwsn_T$, $ackBound$)}
  \tcp{AppendTransact on transaction manager node $i \in [1, n]$} 
  \uIf{$cwsn_T \leq \max(W_i^{\text{key}}|cid)$}{
    \tcp{$W_i[cwsn_T]$ has variant type $\text{resp}|\text{uint}$}
    \uIf{\upshape $i = 1 \wedge W_i[cwsn_T] = \text{resp}(\text{SUCCESS}, data)$}{
      $\text{Send}_{\text{cid}}(\text{SessionRespWrite}, \{cwsn_T, \text{resp}\})$\tcp{handle case when $i$ is head}
    }
    \textbf{return}    
  } \Else {
    \textbf{wait} until $\max(W_i^{\text{key}}|cid).num + 1 = cwsn_T.num$     \tcp{prevent reordering} 
  } 
  $\text{ind}:=\mathcal{L}_i.len$ \\
  $\mathcal{L}_i \leftarrow T$ \\
  $W_i := W_i \cup \{ cwsn_T \mapsto \text{null}\}$ \\
  $W_i := W_i \setminus \{ (k \mapsto W_i[k]) \in W_i : k < ackBound\}$  \tcp{remove unneeded metadata} 
  $C_i := C_i \cup \left \{ind \mapsto (cwsn_T, \{ \}) \right \}$ \\
  \ForEach{$j\in[1,m]$}{
    $\text{particip} := T^{\text{write}} \cap sh(j)$\\
    \uIf{\upshape $\text{particip} \neq \emptyset$}{$Q_i|j \leftarrow \text{ind}$ \\
      $SSN[j] := SSN[j] + 1$\\
      $C_i[\text{ind}].shds := C_i[\text{ind}].shds \cup \{j\}$ 
    }
  }
  \uIf{$i = n$}{
    $\text{ShDeps} := \text{DepAnalysis}(T^{\text{key}})$ \tcp{local and remote dependencies for each participant $j$}
    \ForEach{\upshape$j \in C_i[\text{ind}]$}{
      $\text{Send}_{j}(\text{ShardExecAppend}, \{T|j, \text{ind}, SSN[j], \text{shDeps}[j]\})$ 
    }
  } \Else {
    $\text{Send}_{\text{succ}}(\text{AppendTransact}, \{T, cwsn_T, ackBound\})$ 
  }  
  \textbf{return}
\end{procedure}
\par The check in lines 2-5 is necessary, as a dropped response message can lead to multiple client retries. If the service indiscriminately performs these retries, the resulting execution history might not admit a valid total ordering. 
\par Once the transaction is replicated on the tail node, it is sent to the shard groups for execution. To reduce the size of network messages, only the relevant sub-transaction is transmitted to each participating shard. For transactions whose writes may depend on its read results, the tail also includes the results a read/write set depdendency analysis. This analysis is similar to that presented in Calvin. Retries are piggybacked on subsequent invocations or sent after a preset timeout, whichever happens first. \\
\begin{procedure}[H]
  \caption{ShardExecAppend($T|j$, $ind$, $sn$, $deps$)}
  \SetKwFor{Match}{match}{with:}{end}
  \tcp{ShardExec on shard group $j \in [1, m]$}
  \uIf{$sn \leq ssn_j$}{
    \textbf{return}
  } \Else{
    \textbf{wait} until $ssn_j + 1 = sn$ \tcp{handle manager to shard message reordering}
  }
  \textbf{recv all} $deps^{\text{local}}$\\
  \tcp{Once dependencies resolved, can execute local operations and serve reads}
  $respData := \emptyset$\\
  \ForEach{\upshape$\text{op} \in T|j$}{
    \tcp{Calls to storage service will be batched in practice}
    \Match{\upshape $\text{op}$}{
      $put(key, value) \implies \text{put}(key, value, ind)$ \tcp{versioned put exposed by storage service}
      $get(key) \implies respData := respData \cup \{key \mapsto \text{get}(key)\}$
    }
  }
  \ForEach{$\text{op} \in deps^{\text{remote}}$}{
    $\text{Send}_{\text{op}.shard}(\text{RemoteRead}, \{\text{op}.key \mapsto \text{get}(key)\})$
  }
  $shExec_j := ind$ \\
  $ssn_j := sn$ \\
  $\text{Send}_{\text{tail}}(\text{ExecNotif}, \{j, ind, respData\})$\\
  \textbf{return}
\end{procedure}
\begin{procedure}[H]
  \caption{ExecNotif($j$, $ind$, $respData$)}
  \tcp{ExecNotif on tail node $n$}
  \uIf{$ind \geq exec(Q_n|j)$}{
    $\text{deq} \leftarrow Q_n|j$\\
    \While{\upshape $\text{deq} \neq ind$}{
      $\text{deq} \leftarrow Q_n|j$
    }
  }
  $C_n[ind].shds := C_n[ind].shds\setminus\{ j\}$\\
  $\text{cwsn} := C_n[ind].cwsn$\\
  \uIf{\upshape $W_n[\text{cwsn}] = \text{null}$}{
    $W_n[\text{cwsn}] := \text{resp}(\text{INPROG}, respData)$
  } \Else {
    $W_n[\text{cwsn}].data := W_n[\text{cwsn}].data \cup respData$\\
  }    
  \uIf{$C_n[ind].shds = \emptyset$}{
    \tcp{Local call to start communication backwards through the chain}
    $\text{ExecAppendTransact}(\{ind, W_n[\text{cwsn}].data)$ 
  }
  \textbf{return}
\end{procedure}
\par The tail listens to shard group replies through $\text{ExecNotif}$ and updates its $C_{n}$. After all sub-transactions are finished, transaction completion is propagated backwards to the head through $\text{ExecAppendTransact}$. For each participating shard queue, elements are deqeued up to and including the completed transaction index. This ensures an updated view of all execution statuses, which is critical for correctly serving reads. When the head is done updating its queues, the response is returned to the client session.\\
\begin{procedure}[H]
  \caption{ExecAppendTransact($ind$, $data$)}
  \tcp{ExecAppendTransact on node $i \in [1, n]$}
  $\text{cwsn} := C_i[ind].cwsn$\\
  \ForEach{$j \in C_i[ind].shds : ind \geq exec(Q_i|j)$}{
    $\text{deq} \leftarrow Q_i|j$ \\
    \While{\upshape$\text{deq}\neq ind$}{
      $\text{deq} \leftarrow Q_i | j$
    }
  }
  $C_i := C_i \setminus \{ind \mapsto C_i[ind] \}$\\ 
  $W_i[\text{cwsn}] := \text{resp}(\text{SUCCESS}, data)$\\
  \uIf{$i=1$}{
    $\text{Send}_{\text{cwsn}.cid}(\text{SessionRespWrite}, \{cwsn, W_i[cwsn]\})$
  }
  \Else{
    $\text{Send}_{\text{pred}}(\text{TransasctExec}, \{ind, data\})$ 
  }
  \textbf{return}
\end{procedure}
\begin{procedure}[H]
  \caption{SessionRespWrite($cwsn$, $resp$)}
  \tcp{SessionRespWrite on client $cid$}
  $InProg_{cid} := InProg_{cid}\setminus \{cwsn\}$\\
  \textbf{return} $resp$
\end{procedure} \newpage
\section{Read-Only Transactions} \label{rotxn}
% \par In contrast with write-only transactions, there is no data structure tracking read-only transactions on the transaction manager nodes. Moreover, client sessions . As such, the client-side library plays a non-trivial role in ensuring correctness.
 $\text{SessionRuntimeRead}$ in the client-side library is called on every read-only transaction, including retries. Responses include the log sequence number used in executing the read. \\
\begin{procedure}[H]
  \caption{SessionRuntimeRead($T$, $isRetry$, $crsn_T$ $|$ null)}
  \tcp{SessionRuntimeRead on client $cid$, connected to manager node $i$}
  $writeDep, lsnConst := -1$ \\
  $isRetry := False$   \\
  \uIf{$InProg_{cid} \neq \emptyset$}{
    $writeDep := \max(InProg_{cid})$
  }
  \tcp{Compute invocation order constraints}
  \uIf{$crsn_T = \text{null}$}{
    $crsn_{\max} := crsn_{\max} + 1$ \\
    $crsn_T := crsn_{\max}$\\
    $ReadResult_{cid} := ReadResult_{cid} \cup \{ crsn_T \mapsto (-1, \{ \}, lsnConst)\}$\\
    \uIf{$|ReadResult_{cid}| > 1$}{$UB_{cid} := UB_{cid} \cup \{crsn_T\mapsto \text{null} \}$}
  } \Else {
    $isRetry := True$ \\
    \tcp{Check existence of upper bound} 
    $\text{ubKey} := \min\{ x \in UB_{cid}^{\text{key}} : crsn_T\leq x \wedge UB_{cid}[x] \neq \text{null}\}$ \\
    \uIf{\upshape$\text{ubKey}\neq \text{null}$}{
      $lsnConst := UB_{cid}[\text{ubKey}]$ 
    }
    $ReadResult_{cid}[crsn_T] := (-1, \{ \}, lsnConst)$ \tcp{throw away partially completed read}
  }
  %$ackBound := \min(InProg_{cid})$ \tcp{also piggyback write acknowledgements on reads}
  $\text{Send}_{i}(\text{ReadOnlyTransact}, \{T, isRetry, crsn_T, writeDep, lsnConst\})$ \\
  \textbf{return} $crsn_T$
\end{procedure}
The utility of $crsn_T$ is the same as in write-only transactions. For every read-only transaction, $writeDep$ captures dependency on the most recent ongoing write. This is not required in all cases. If the write key-set of all ongoing transactions are known, a dependency need only be declared on the latest transaction $X$ such that $X^{\text{write}} \cap T^{\text{read}} \neq \emptyset$. Unfortunately, this optimization is not always possible due to the way we handle read-write and dependent transactions (\S\ref{deptxn}). \\
\begin{procedure}[H]
  \caption{SessionRespRead($data$, $crsn_T$, $fence$, $numShards$)}
  \tcp{SessionRespRead on client $cid$}
  $(\text{currNum}, \text{currData}, \text{currLsn}) := ReadResult_{cid}[crsn_T]$ \\
  \uIf{\upshape $\text{currLsn} = -1 \lor \text{currLsn} = fence$}{
    \uIf{\upshape$ \text{currNum} = -1$}{
      $\text{currNum} := numShards$
    } 
    $\text{currNum} := \text{currNum} - 1$\\
    $\text{currData} := \text{currData} \cup \{data\}$ \\
    \tcp{Recieved data from all shards, so can return to client}  
    \uIf{\upshape $\text{currNum} = 0$}{
      $ReadResult_{cid} := ReadResult_{cid} \setminus \{ crsn_T \mapsto ReadResult_{cid}[crsn_T]\}$ \\
      \uIf{\upshape $crsn_T \in UB_{cid}^{\text{key}}$}{
        $UB_{cid}[crsn_T] := lsn$\\
        \uIf{$crsn_T \neq crsn_{\max}$}{
          $UB_{cid} := UB_{cid}\setminus \{(crsn_T + 1) \mapsto UB_{cid}[crsn_T + 1]\}$
        }
      }
      \textbf{return} $data$
    } \Else{
      $ReadResult_{cid}[crsn_T] := (\text{currNum}, \text{currData}, \text{currLsn})$
    }
  }
\end{procedure}
\par For retries, the client session must also explicitly provide an upper bound on freshness, $lsnConst$, to the transaction manager node. Otherwise, a retry could return information newer than that of later reads, with respect to invocation order. Consider the set of completed read-only transactions with CRSN greater than $crsn_T$. We take $lsnConst$ as the log sequence number contained in the response to the minimal element of this set. That is, $lsnConst$ can be thought of as the \underline{least} upper bound on freshness. In practice, we do not track all completed read-only transactions. The set $UB_{cid}$ only contains least upper bounds, which corresponds to transactions whose immediate predecessors have not yet received a response (lines 9-12 of SessionRespRead). 
\par Once a read is retried, responses matching the original invocation must be discarded. Again, this is needed to prevent more recent invocations from matching with a response containing data more stale than older invocations. Observe that this scheme can potentially lead to a large amount of unneccesary, cascading retries. For example, responses for a large set of read transactions may arrive just after all retries are sent out, thereby invalidating their content. However, we expect that manifestation of these behaviors is highly unlikely, assuming a resonable timeout value. 
\par Based on the status of their queues, transaction manager nodes compute a consistent read across shards subject to the client library's $writeDep$ and $lsnConstr$ constraints. This consistent read can be visualized as a ``fence'' that cuts across the manager's queues.\\
\begin{procedure}[H]
  \caption{ReadOnlyTransact($T$, $isRetry$, $crsn_T$, $writeDep$, $lsnConst$)}
  \SetKwFor{Match}{match}{with:}{end}
  \tcp{ReadOnlyTransact on manager node $i\in[1,n]$}
  \uIf{$ writeDep \geq\min(W_i^{\text{key}}|cid).num$}{
    \textbf{wait} until $writeDep \in W_i^{\text{key}}$ or $\text{ABORT}(writeDep) $ 
  }
  $fence := -1$\\
  $\text{ShardSet} := \emptyset$\\
  \tcp{Participating shards and preliminary fence computation}
  \ForEach{$j \in [1,m]$}{
    $\text{particip} := T^{\text{read}} \cap sh(j)$\\
    \uIf{\upshape $\text{particip} \neq \emptyset$}{
      $\text{ShardSet} := \text{ShardSet} \cup \{ j\}$\\
      $fence := \max\{fence, exec(Q_i|j)\}$                
    }
  }
  \tcp{Modify constraints based on retry flag and $R_i$}
  \uIf{$isRetry$}{
    $fence := lsnConst$
  }
  \uIf{\upshape $crsn_T.cid \in R_i^{\text{key}}$}{
    \uIf{$crsn_T.num \leq R_i[crsn_T.cid].num$}{
      \uIf{$\neg isRetry$}{
        $fence := R_i[crsn_T.cid].lsn$
      }
    } \Else{
      \uIf{$\neg isRetry$}{
        $fence := \max\{fence, R_i[crsn_T.cid].lsn\}$
      }
      $R_i[crsn_T.cid] := fence$
    }    
  } \Else{
    $R_i := R_i \cup \{ crsn_T.cid \mapsto fence \}$
  }
  \tcp{Sends reads and constraints to shards}
  $numShards := |\text{ShardSet}|$\\
  \ForEach{$j \in \text{ShardSet}$}{
    $\text{Send}_{j}(\text{ShardExecRead}, \{T|j, crsn_T, fence, numShards\})$
  }
  \textbf{return}
\end{procedure}
\par A shard group simply waits until the manager-determined constraint is satisfied before executing its portion of the read. Shards return directly to the client, $crsn_T.cid$, which originated the request. Because correctness depends only on execution up to a specified log sequence number, writes on the shard can continue uninterrupted in parallel.\\
\begin{procedure}[H]
  \caption{ShardExecRead($T|j, crsn_T, fence, numShards$)}
  \tcp{ShardExecRead on shard group $j$}
  \textbf{wait} until $shExec_j \geq fence$\\
  $data := \emptyset$\\
  \ForEach{\upshape $\text{op} \in T|j$}{
    \tcp{multi-versioned get exposed by storage service, returns value with greatest version less than $fence$}
    \tcp{as before, calls batched in practice}
    $data := data \cup \{\text{op}.key \mapsto \text{get}(\text{op}.key, fence)\}$
  }
  $\text{Send}_{crsn_{T}.cid }(\text{SessionRespRead}, \{data, crsn_T, fence, numShards\})$\\
  \textbf{return}
\end{procedure}


\section{Dependent Read-Write Transactions} \label{deptxn}
In constrast to the simple case, a transactions is \emph{dependent} if the full read/write key-set depends on some of its own reads. A scenario where this can arise is secondary index lookups. We buffer these types of transactions at the head node, which fully reduces the transaction into a simple read-write transaction. More specifically, the head node makes the necessary read requests to satisfy all intra-transaction dependencies. For deduplication, the head maintains a set $BF$ of all buffered transactions, in addition to the state described in figure \ref{state}. A more client-driven approach would enable the $writeDep$ optimization described in the previous section, but the tradeoff would be an increase in the number of client to manager node round trips, which runs contrary to our goal of acheiving higher throughput.
\par Unfortunately, contention and aborts are possible when clients elect to use these types of transaction. Since the reduction process involves one or more reads, any write that affects the read keys must trigger a transaction abort. Otherwise, we would introduce safety violation. We use a priority-based mechnism so that dependent transactions are not constantly aborted in times of high contention. When a transaction aborts, the head sends a notification down the chain, enabling constraint removal on affected reads (line 2 of ReadOnlyTransact). To submit a dependent read-write transaction, the client uses a slightly modified SessionRuntimeAppend, with ReduceTransact replacing AppendTransact in the call on line 5. \\
\begin{procedure}[H]
  \caption{ReduceTransact($T$, $cwsn_T$, $ackBound$)}
  \uIf{$cwsn_T \in BF$}{
    \textbf{return}
  }
  $BF := BF \cup {cwsn_T}$\\
  $\text{toRead} := \text{DepAnalysis}(T^{\text{key}})$ \tcp{get read dependencies}
  $\text{depResults} := \emptyset$\\
  $W_i := W_i \setminus \{ (k \mapsto W_i[k]) \in W_i : k < ackBound\}$  \tcp{remove unneeded metadata} 
  \ForEach{\upshape$j, \text{ops} \in \text{toRead}$}{
    $\text{Send}_{j}(\text{ShardExecHeadRead}, \{ \text{ops}, \text{tail}(Q_1|j)\})$
  }
  \uIf{\upshape$\text{ABORT}(cwsn_T)$}{
    $BF := BF\setminus{cwsn_T}$\\
    $\text{Send}_{cwsn_T.cid}(\text{SessionRespWrite}, \{cwsn_T, (\text{ABORT}, \emptyset)\})$\\
    $\text{Send}_{\text{succ}}(\text{NotifAbort}, \{ cwsn_T, ackBound\})$\\
    \textbf{return}
  }\Else{
    $\textbf{recv all } \text{depResults}$
  }
  $T_{\text{reduce}} := T[\text{depResults}]$ \tcp{resolve read/write set}
  $\text{AppendTransact}(T_{\text{reduce}}, cwsn_T, ackBound)$
\end{procedure}




% Talk about aborts being sent through chain to clear any reads


% \section{Checkpointing and metadata garbage collection}
% % Need to talk about garbage collection of these values
% Lower bounds over all clients $lb$ (need to track possible $lsnConst$ values, should make $maxNum$ a range instead). Routine garbage sweeps alternating between head $\rightarrow$ tail and tail $\rightarrow$ head (piggyback on append and exec). Log trimming on committed portion of log -- everything before $\min(\min(C_i^{\text{key}}), lb)$.
% \section{Failure Recovery}
% TODO: Need to talk about failure recovery for outstanding reads and outstanding writes. Aborts replicated for performance on head failover.
% In failure case, abort needs to send down the chain by new head

% Client first performs read before issuing write. On tail node failover, any committed but not yet executed transactions must be re-processed by the new tail. Since writes are predetermined and buffered at client side, applying the unexecuted suffix of the log is an idempotent operation for all shards.

\end{document}
